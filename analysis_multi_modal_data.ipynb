{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinsukim/miniconda3/envs/research-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "kstar_shot_list = pd.read_csv('./dataset/KSTAR_Disruption_Shot_List_extend.csv', encoding = \"euc-kr\")\n",
    "ts_data = pd.read_csv(\"./dataset/KSTAR_Disruption_ts_data_for_multi.csv\")\n",
    "mult_info = pd.read_csv(\"./dataset/KSTAR_Disruption_multi_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>\\q95</th>\n",
       "      <th>\\ipmhd</th>\n",
       "      <th>\\kappa</th>\n",
       "      <th>\\tritop</th>\n",
       "      <th>\\tribot</th>\n",
       "      <th>\\betap</th>\n",
       "      <th>\\betan</th>\n",
       "      <th>\\li</th>\n",
       "      <th>\\WTOT_DLM03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.135000</td>\n",
       "      <td>4.451424</td>\n",
       "      <td>-517687.502496</td>\n",
       "      <td>1.592205</td>\n",
       "      <td>0.290356</td>\n",
       "      <td>0.669179</td>\n",
       "      <td>0.604620</td>\n",
       "      <td>0.798851</td>\n",
       "      <td>1.393783</td>\n",
       "      <td>113.760088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.154048</td>\n",
       "      <td>4.451229</td>\n",
       "      <td>-517758.959331</td>\n",
       "      <td>1.592658</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.669380</td>\n",
       "      <td>0.604886</td>\n",
       "      <td>0.799086</td>\n",
       "      <td>1.394316</td>\n",
       "      <td>113.898366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.173095</td>\n",
       "      <td>4.451524</td>\n",
       "      <td>-517773.000585</td>\n",
       "      <td>1.593156</td>\n",
       "      <td>0.290629</td>\n",
       "      <td>0.669634</td>\n",
       "      <td>0.606237</td>\n",
       "      <td>0.800684</td>\n",
       "      <td>1.393948</td>\n",
       "      <td>114.889598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.192143</td>\n",
       "      <td>4.451977</td>\n",
       "      <td>-517768.464867</td>\n",
       "      <td>1.593668</td>\n",
       "      <td>0.290753</td>\n",
       "      <td>0.669905</td>\n",
       "      <td>0.607941</td>\n",
       "      <td>0.802722</td>\n",
       "      <td>1.393288</td>\n",
       "      <td>116.156805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.211190</td>\n",
       "      <td>4.449676</td>\n",
       "      <td>-517497.617650</td>\n",
       "      <td>1.593433</td>\n",
       "      <td>0.288556</td>\n",
       "      <td>0.670172</td>\n",
       "      <td>0.607891</td>\n",
       "      <td>0.802569</td>\n",
       "      <td>1.395915</td>\n",
       "      <td>116.355835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time      \\q95         \\ipmhd    \\kappa   \\tritop   \\tribot    \\betap  \\\n",
       "0  3.135000  4.451424 -517687.502496  1.592205  0.290356  0.669179  0.604620   \n",
       "1  3.154048  4.451229 -517758.959331  1.592658  0.290501  0.669380  0.604886   \n",
       "2  3.173095  4.451524 -517773.000585  1.593156  0.290629  0.669634  0.606237   \n",
       "3  3.192143  4.451977 -517768.464867  1.593668  0.290753  0.669905  0.607941   \n",
       "4  3.211190  4.449676 -517497.617650  1.593433  0.288556  0.670172  0.607891   \n",
       "\n",
       "     \\betan       \\li  \\WTOT_DLM03  \n",
       "0  0.798851  1.393783   113.760088  \n",
       "1  0.799086  1.394316   113.898366  \n",
       "2  0.800684  1.393948   114.889598  \n",
       "3  0.802722  1.393288   116.156805  \n",
       "4  0.802569  1.395915   116.355835  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_start</th>\n",
       "      <th>frame_end</th>\n",
       "      <th>is_disrupt</th>\n",
       "      <th>shot</th>\n",
       "      <th>task</th>\n",
       "      <th>path</th>\n",
       "      <th>t_start</th>\n",
       "      <th>t_end</th>\n",
       "      <th>t_start_index</th>\n",
       "      <th>t_end_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>656</td>\n",
       "      <td>740</td>\n",
       "      <td>False</td>\n",
       "      <td>21273</td>\n",
       "      <td>train</td>\n",
       "      <td>./dataset/dur84_dis4/train/normal/21273_656_740</td>\n",
       "      <td>3.115953</td>\n",
       "      <td>3.515952</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740</td>\n",
       "      <td>824</td>\n",
       "      <td>False</td>\n",
       "      <td>21273</td>\n",
       "      <td>valid</td>\n",
       "      <td>./dataset/dur84_dis4/valid/normal/21273_740_824</td>\n",
       "      <td>3.515952</td>\n",
       "      <td>3.915952</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>824</td>\n",
       "      <td>908</td>\n",
       "      <td>False</td>\n",
       "      <td>21273</td>\n",
       "      <td>train</td>\n",
       "      <td>./dataset/dur84_dis4/train/normal/21273_824_908</td>\n",
       "      <td>3.915952</td>\n",
       "      <td>4.315952</td>\n",
       "      <td>42</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>908</td>\n",
       "      <td>992</td>\n",
       "      <td>False</td>\n",
       "      <td>21273</td>\n",
       "      <td>train</td>\n",
       "      <td>./dataset/dur84_dis4/train/normal/21273_908_992</td>\n",
       "      <td>4.315952</td>\n",
       "      <td>4.715952</td>\n",
       "      <td>63</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>992</td>\n",
       "      <td>1076</td>\n",
       "      <td>False</td>\n",
       "      <td>21273</td>\n",
       "      <td>train</td>\n",
       "      <td>./dataset/dur84_dis4/train/normal/21273_992_1076</td>\n",
       "      <td>4.715952</td>\n",
       "      <td>5.115952</td>\n",
       "      <td>84</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_start  frame_end  is_disrupt   shot   task  \\\n",
       "0          656        740       False  21273  train   \n",
       "1          740        824       False  21273  valid   \n",
       "2          824        908       False  21273  train   \n",
       "3          908        992       False  21273  train   \n",
       "4          992       1076       False  21273  train   \n",
       "\n",
       "                                               path   t_start     t_end  \\\n",
       "0   ./dataset/dur84_dis4/train/normal/21273_656_740  3.115953  3.515952   \n",
       "1   ./dataset/dur84_dis4/valid/normal/21273_740_824  3.515952  3.915952   \n",
       "2   ./dataset/dur84_dis4/train/normal/21273_824_908  3.915952  4.315952   \n",
       "3   ./dataset/dur84_dis4/train/normal/21273_908_992  4.315952  4.715952   \n",
       "4  ./dataset/dur84_dis4/train/normal/21273_992_1076  4.715952  5.115952   \n",
       "\n",
       "   t_start_index  t_end_index  \n",
       "0              0           20  \n",
       "1             21           41  \n",
       "2             42           62  \n",
       "3             63           83  \n",
       "4             84          104  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shot</th>\n",
       "      <th>year</th>\n",
       "      <th>tftsrt</th>\n",
       "      <th>tipminf</th>\n",
       "      <th>tTQend</th>\n",
       "      <th>dt</th>\n",
       "      <th>frame_cutoff</th>\n",
       "      <th>frame_tTQend</th>\n",
       "      <th>frame_tipminf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21273</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.996</td>\n",
       "      <td>5.535</td>\n",
       "      <td>5.514</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1165</td>\n",
       "      <td>1160</td>\n",
       "      <td>1164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21274</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.996</td>\n",
       "      <td>10.056</td>\n",
       "      <td>10.038</td>\n",
       "      <td>0.018</td>\n",
       "      <td>2104</td>\n",
       "      <td>2100</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21310</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.500</td>\n",
       "      <td>5.368</td>\n",
       "      <td>5.342</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1131</td>\n",
       "      <td>1125</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21315</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.500</td>\n",
       "      <td>7.804</td>\n",
       "      <td>7.782</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1636</td>\n",
       "      <td>1631</td>\n",
       "      <td>1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21317</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.500</td>\n",
       "      <td>9.460</td>\n",
       "      <td>9.438</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1980</td>\n",
       "      <td>1975</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    shot  year  tftsrt  tipminf  tTQend     dt  frame_cutoff  frame_tTQend  \\\n",
       "0  21273  2018   2.996    5.535   5.514  0.021          1165          1160   \n",
       "1  21274  2018   2.996   10.056  10.038  0.018          2104          2100   \n",
       "2  21310  2018   1.500    5.368   5.342  0.026          1131          1125   \n",
       "3  21315  2018   1.500    7.804   7.782  0.022          1636          1631   \n",
       "4  21317  2018   1.500    9.460   9.438  0.022          1980          1975   \n",
       "\n",
       "   frame_tipminf  \n",
       "0           1164  \n",
       "1           2103  \n",
       "2           1130  \n",
       "3           1635  \n",
       "4           1979  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kstar_shot_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_video :  torch.Size([32, 3, 21, 128, 128])\n",
      "sample_0D :  torch.Size([32, 21, 9])\n",
      "sample_target :  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Optional, Literal, List, Union\n",
    "from tqdm.auto import tqdm\n",
    "from src.CustomDataset import DEFAULT_TS_COLS\n",
    "import os, cv2\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        task : Literal[\"train\", \"valid\", \"test\"] = \"train\", \n",
    "        ts_data : Optional[pd.DataFrame] = None,\n",
    "        ts_cols : Optional[List] = None,\n",
    "        mult_info : Optional[pd.DataFrame] = None,\n",
    "        dt : Optional[float] = 1.0 / 210 * 4,\n",
    "        distance : Optional[int] = 0,\n",
    "        n_fps : Optional[int] = 4,\n",
    "        resize_height : Optional[int] = 256,\n",
    "        resize_width : Optional[int] = 256,\n",
    "        crop_size : Optional[int] = 128,\n",
    "        seq_len : int = 21,\n",
    "        n_classes : int = 2,\n",
    "        ):\n",
    "        self.task = task # task : train / valid / test \n",
    "        \n",
    "        # resize each frame from video\n",
    "        self.resize_height = resize_height\n",
    "        self.resize_width = resize_width\n",
    "        \n",
    "        # crop\n",
    "        self.crop_size = crop_size\n",
    "        \n",
    "        # video sequence length\n",
    "        # warning : 0D data and video data should have equal sequence length\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # use for 0D data prediction\n",
    "        self.distance = distance # prediction time\n",
    "        self.dt = dt # time difference of 0D data\n",
    "        self.n_fps = n_fps\n",
    "\n",
    "        # video_file_path : video file path : {database}/{shot_num}_{frame_start}_{frame_end}.avi\n",
    "        # indices : index for tabular data, shot == shot_num, index <- df[df.frame_idx == frame_start].index\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.ts_data = ts_data\n",
    "        self.mult_info = mult_info\n",
    "        self.ts_cols = ts_cols\n",
    "        \n",
    "        # select columns for 0D data prediction\n",
    "        if ts_cols is None:\n",
    "            self.ts_cols = DEFAULT_TS_COLS\n",
    "            \n",
    "        self.video_file_path = mult_info[mult_info.task == task][\"path\"].values.tolist()\n",
    "        self.labels = [0 if label is True else 1 for label in mult_info[mult_info.task == task].is_disrupt]\n",
    "        self.indices = mult_info[mult_info.task == task][\"t_start_index\"].astype(int).values.tolist()\n",
    "\n",
    "    def load_frames(self, file_dir : str):\n",
    "        frames = sorted([os.path.join(file_dir, img) for img in os.listdir(file_dir)])\n",
    "        frame_count = self.seq_len\n",
    "        buffer = np.empty((frame_count, self.resize_height, self.resize_width, 3), np.dtype('float32'))\n",
    "        \n",
    "        for i, frame_name in enumerate(frames[::-1][::self.n_fps][::-1]):\n",
    "            frame = np.array(cv2.imread(frame_name)).astype(np.float32)\n",
    "            buffer[i] = frame\n",
    "    \n",
    "        return buffer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx : int):\n",
    "        x_video = self.get_video_data(idx)\n",
    "        x_tabular = self.get_tabular_data(idx)\n",
    "        label = torch.from_numpy(np.array(self.labels[idx]))\n",
    "        return x_video, x_tabular, label\n",
    "\n",
    "    def get_video_data(self, index : int):\n",
    "        buffer = self.load_frames(self.video_file_path[index])\n",
    "        if buffer.shape[0] < self.seq_len:\n",
    "            buffer = self.refill_temporal_slide(buffer)\n",
    "        buffer = self.crop(buffer, self.seq_len, self.crop_size)\n",
    "        buffer = self.normalize(buffer)\n",
    "        buffer = self.to_tensor(buffer)\n",
    "        return torch.from_numpy(buffer)\n",
    "    \n",
    "    def get_tabular_data(self, index : int):\n",
    "        ts_idx = self.indices[index]\n",
    "        data = self.ts_data[self.ts_cols].loc[ts_idx:ts_idx+self.seq_len-1].values\n",
    "        return torch.from_numpy(data)\n",
    "\n",
    "    def refill_temporal_slide(self, buffer:np.ndarray):\n",
    "        for _ in range(self.seq_len - buffer.shape[0]):\n",
    "            frame_new = buffer[-1].reshape(1, self.resize_height, self.resize_width, 3)\n",
    "            buffer = np.concatenate((buffer, frame_new))\n",
    "        return buffer\n",
    "\n",
    "    def normalize(self, buffer):\n",
    "        for i, frame in enumerate(buffer):\n",
    "            frame -= np.array([[[90.0, 98.0, 102.0]]])\n",
    "            buffer[i] = frame\n",
    "        return buffer\n",
    "\n",
    "    def to_tensor(self, buffer:Union[np.ndarray, torch.Tensor]):\n",
    "        return buffer.transpose((3, 0, 1, 2))\n",
    "\n",
    "    def crop(self, buffer : Union[np.ndarray, torch.Tensor], clip_len : int, crop_size : int, is_random : bool = False):\n",
    "        if buffer.shape[0] < clip_len :\n",
    "            time_index = np.random.randint(abs(buffer.shape[0] - clip_len))\n",
    "        elif buffer.shape[0] == clip_len :\n",
    "            time_index = 0\n",
    "        else :\n",
    "            time_index = np.random.randint(buffer.shape[0] - clip_len)\n",
    "\n",
    "        if not is_random:\n",
    "            original_height = self.resize_height\n",
    "            original_width = self.resize_width\n",
    "            mid_x, mid_y = original_height // 2, original_width // 2\n",
    "            offset_x, offset_y = crop_size // 2, crop_size // 2\n",
    "            buffer = buffer[time_index : time_index + clip_len, mid_x - offset_x:mid_x+offset_x, mid_y - offset_y: mid_y+ offset_y, :]\n",
    "        else:\n",
    "            height_index = np.random.randint(buffer.shape[1] - crop_size)\n",
    "            width_index = np.random.randint(buffer.shape[2] - crop_size)\n",
    "\n",
    "            buffer = buffer[time_index:time_index + clip_len,\n",
    "                    height_index:height_index + crop_size,\n",
    "                    width_index:width_index + crop_size, :]\n",
    "        return buffer\n",
    "\n",
    "    # function for imbalanced dataset\n",
    "    # used for LDAM loss and re-weighting\n",
    "    def get_num_per_cls(self):\n",
    "        classes = np.unique(self.labels)\n",
    "        self.num_per_cls_dict = dict()\n",
    "\n",
    "        for cls in classes:\n",
    "            num = np.sum(np.where(self.labels == cls, 1, 0))\n",
    "            self.num_per_cls_dict[cls] = num\n",
    "         \n",
    "    def get_cls_num_list(self):\n",
    "        cls_num_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            cls_num_list.append(self.num_per_cls_dict[i])\n",
    "        return cls_num_list\n",
    "    \n",
    "\n",
    "train_data = MultiModalDataset('train', ts_data, DEFAULT_TS_COLS, mult_info, dt = 1 / 210 * 4, distance = 4, seq_len = 21)\n",
    "valid_data = MultiModalDataset('valid', ts_data, DEFAULT_TS_COLS, mult_info, dt = 1 / 210 * 4, distance = 4, seq_len = 21)\n",
    "test_data = MultiModalDataset('test', ts_data, DEFAULT_TS_COLS, mult_info, dt = 1 / 210 * 4, distance = 4, seq_len = 21)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils.sampler import ImbalancedDatasetSampler\n",
    "\n",
    "batch_size = 32\n",
    "sampler = ImbalancedDatasetSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, num_workers = 8, sampler = sampler)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, num_workers = 8, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, num_workers = 8, shuffle = True)\n",
    "\n",
    "sample_video, sample_0D, sample_target = next(iter(train_loader))\n",
    "print(\"sample_video : \", sample_video.size())\n",
    "print(\"sample_0D : \", sample_0D.size())\n",
    "print(\"sample_target : \", sample_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m model(sample_video\u001b[39m.\u001b[39;49mto(device), sample_0D\u001b[39m.\u001b[39;49mto(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes_for_study/research-predict-disruption/src/models/mult_modal.py:26\u001b[0m, in \u001b[0;36mMultiModalModel.forward\u001b[0;34m(self, x_video, x_0D)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_video : torch\u001b[39m.\u001b[39mTensor, x_0D : torch\u001b[39m.\u001b[39mTensor)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mtorch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     25\u001b[0m     x_video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_video(x_video)\n\u001b[0;32m---> 26\u001b[0m     x_0D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_0D(x_0D)\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x_video, x_0D], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes_for_study/research-predict-disruption/src/models/ConvLSTM.py:133\u001b[0m, in \u001b[0;36mConvLSTMEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x : torch\u001b[39m.\u001b[39mTensor)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mtorch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 133\u001b[0m     x_conv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    134\u001b[0m     h_0 \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m, x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_dim))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    135\u001b[0m     c_0 \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m, x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_dim))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:301\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:297\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    295\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    296\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 297\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    298\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "model(sample_video.to(device), sample_0D.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Literal, Union\n",
    "from src.loss import LDAMLoss, FocalLoss\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_per_epoch(\n",
    "    train_loader : DataLoader, \n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    scheduler : Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "    loss_fn : torch.nn.Module,\n",
    "    device : str = \"cpu\",\n",
    "    max_norm_grad : Optional[float] = None\n",
    "    ):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    total_pred = np.array([])\n",
    "    total_label = np.array([])\n",
    "    total_size = 0\n",
    "\n",
    "    for batch_idx, (x_video, x_0D, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_video = x_video.to(device)\n",
    "        x_0D = x_0D.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        print(\"x_video : \", x_video.size())\n",
    "        print(\"x_0D : \", x_0D.size())\n",
    "        print(\"target : \", target.size())\n",
    "    \n",
    "        output = model(x_video, x_0D)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        print(\"output : \", output.size())\n",
    "\n",
    "        # use gradient clipping\n",
    "        if max_norm_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pred = torch.nn.functional.softmax(output, dim = 1).max(1, keepdim = True)[1]\n",
    "        train_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total_size += x_video.size(0) \n",
    "        \n",
    "        total_pred = np.concatenate((total_pred, pred.cpu().numpy().reshape(-1,)))\n",
    "        total_label = np.concatenate((total_label, target.cpu().numpy().reshape(-1,)))\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss /= total_size\n",
    "    train_acc /= total_size\n",
    "\n",
    "    train_f1 = f1_score(total_label, total_pred, average = \"macro\")\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "def valid_per_epoch(\n",
    "    valid_loader : DataLoader, \n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    loss_fn : torch.nn.Module,\n",
    "    device : str = \"cpu\",\n",
    "    ):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "\n",
    "    total_pred = np.array([])\n",
    "    total_label = np.array([])\n",
    "    total_size = 0\n",
    "\n",
    "    for batch_idx, (x_video, x_0D, target) in enumerate(valid_loader):\n",
    "        with torch.no_grad():\n",
    "            optimizer.zero_grad()\n",
    "            x_video = x_video.to(device)\n",
    "            x_0D = x_0D.to(device)\n",
    "            target = target.to(device)\n",
    "        \n",
    "            output = model(x_video, x_0D)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "    \n",
    "            valid_loss += loss.item()\n",
    "            pred = torch.nn.functional.softmax(output, dim = 1).max(1, keepdim = True)[1]\n",
    "            valid_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total_size += x_video.size(0) \n",
    "\n",
    "            total_pred = np.concatenate((total_pred, pred.cpu().numpy().reshape(-1,)))\n",
    "            total_label = np.concatenate((total_label, target.cpu().numpy().reshape(-1,)))\n",
    "\n",
    "    valid_loss /= total_size\n",
    "    valid_acc /= total_size\n",
    "\n",
    "    valid_f1 = f1_score(total_label, total_pred, average = \"macro\")\n",
    "\n",
    "    return valid_loss, valid_acc, valid_f1\n",
    "\n",
    "def train(\n",
    "    train_loader : DataLoader, \n",
    "    valid_loader : DataLoader,\n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    scheduler : Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "    loss_fn : Union[torch.nn.CrossEntropyLoss, LDAMLoss, FocalLoss],\n",
    "    device : str = \"cpu\",\n",
    "    num_epoch : int = 64,\n",
    "    verbose : Optional[int] = 8,\n",
    "    save_best_dir : str = \"./weights/best.pt\",\n",
    "    save_last_dir : str = \"./weights/last.pt\",\n",
    "    max_norm_grad : Optional[float] = None,\n",
    "    criteria : Literal[\"f1_score\", \"acc\", \"loss\"] = \"f1_score\",\n",
    "    ):\n",
    "\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    \n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "\n",
    "    train_f1_list = []\n",
    "    valid_f1_list = []\n",
    "\n",
    "    best_acc = 0\n",
    "    best_epoch = 0\n",
    "    best_f1 = 0\n",
    "    best_loss = torch.inf\n",
    "\n",
    "    for epoch in tqdm(range(num_epoch), desc = \"training process\"):\n",
    "\n",
    "        train_loss, train_acc, train_f1 = train_per_epoch(\n",
    "            train_loader, \n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            max_norm_grad\n",
    "        )\n",
    "\n",
    "        valid_loss, valid_acc, valid_f1 = valid_per_epoch(\n",
    "            valid_loader, \n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            device \n",
    "        )\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        train_f1_list.append(train_f1)\n",
    "        valid_f1_list.append(valid_f1)\n",
    "\n",
    "        if verbose:\n",
    "            if epoch % verbose == 0:\n",
    "                print(\"epoch : {}, train loss : {:.3f}, valid loss : {:.3f}, train acc : {:.3f}, valid acc : {:.3f}, train f1 : {:.3f}, valid f1 : {:.3f}\".format(\n",
    "                    epoch+1, train_loss, valid_loss, train_acc, valid_acc, train_f1, valid_f1\n",
    "                ))\n",
    "\n",
    "        # save the best parameters\n",
    "        \n",
    "        if criteria == \"acc\" and best_acc < valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_f1 = valid_f1\n",
    "            best_loss = valid_loss\n",
    "            best_epoch  = epoch\n",
    "            torch.save(model.state_dict(), save_best_dir)\n",
    "        elif criteria == \"f1_score\" and best_f1 < valid_f1:\n",
    "            best_acc = valid_acc\n",
    "            best_f1 = valid_f1\n",
    "            best_loss = valid_loss\n",
    "            best_epoch  = epoch\n",
    "            torch.save(model.state_dict(), save_best_dir)\n",
    "        elif criteria == \"loss\" and best_loss > valid_loss:\n",
    "            best_acc = valid_acc\n",
    "            best_f1 = valid_f1\n",
    "            best_loss = valid_loss\n",
    "            best_epoch  = epoch\n",
    "            torch.save(model.state_dict(), save_best_dir)\n",
    "\n",
    "        # save the last parameters\n",
    "        torch.save(model.state_dict(), save_last_dir)\n",
    "\n",
    "    # print(\"\\n============ Report ==============\\n\")\n",
    "    print(\"training process finished, best loss : {:.3f} and best acc : {:.3f}, best f1 : {:.3f}, best epoch : {}\".format(\n",
    "        best_loss, best_acc, best_f1, best_epoch\n",
    "    ))\n",
    "\n",
    "    return  train_loss_list, train_acc_list, train_f1_list,  valid_loss_list,  valid_acc_list, valid_f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "        Layer (type)              Input Shape         Param #     Tr. Param #\n",
      "==============================================================================\n",
      "      ViViTEncoder-1     [1, 3, 21, 128, 128]       1,535,744       1,535,744\n",
      "   ConvLSTMEncoder-2               [1, 21, 9]          61,088          61,088\n",
      "            Linear-3                 [1, 192]          18,528          18,528\n",
      "       BatchNorm1d-4                  [1, 96]             192             192\n",
      "              ReLU-5                  [1, 96]               0               0\n",
      "            Linear-6                  [1, 96]             194             194\n",
      "==============================================================================\n",
      "Total params: 1,615,746\n",
      "Trainable params: 1,615,746\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training process:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_video :  torch.Size([32, 3, 21, 128, 128])\n",
      "x_0D :  torch.Size([32, 21, 9])\n",
      "target :  torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training process:   0%|          | 0/64 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=49'>50</a>\u001b[0m criteria \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=50'>51</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m lr)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=52'>53</a>\u001b[0m train_loss, train_acc, train_f1, valid_loss, valid_acc, valid_f1 \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=53'>54</a>\u001b[0m     train_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=54'>55</a>\u001b[0m     valid_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=55'>56</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=56'>57</a>\u001b[0m     optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=57'>58</a>\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=58'>59</a>\u001b[0m     loss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=59'>60</a>\u001b[0m     device,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=60'>61</a>\u001b[0m     num_epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=61'>62</a>\u001b[0m     verbose,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=62'>63</a>\u001b[0m     save_best_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=63'>64</a>\u001b[0m     save_last_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=64'>65</a>\u001b[0m     max_norm_grad,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=65'>66</a>\u001b[0m     criteria\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=66'>67</a>\u001b[0m )\n",
      "\u001b[1;32m/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, optimizer, scheduler, loss_fn, device, num_epoch, verbose, save_best_dir, save_last_dir, max_norm_grad, criteria)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=141'>142</a>\u001b[0m best_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39minf\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=143'>144</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_epoch), desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtraining process\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=145'>146</a>\u001b[0m     train_loss, train_acc, train_f1 \u001b[39m=\u001b[39m train_per_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=146'>147</a>\u001b[0m         train_loader, \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=147'>148</a>\u001b[0m         model,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=148'>149</a>\u001b[0m         optimizer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=149'>150</a>\u001b[0m         scheduler,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=150'>151</a>\u001b[0m         loss_fn,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=151'>152</a>\u001b[0m         device,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=152'>153</a>\u001b[0m         max_norm_grad\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=153'>154</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=155'>156</a>\u001b[0m     valid_loss, valid_acc, valid_f1 \u001b[39m=\u001b[39m valid_per_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=156'>157</a>\u001b[0m         valid_loader, \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=157'>158</a>\u001b[0m         model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=160'>161</a>\u001b[0m         device \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=161'>162</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=163'>164</a>\u001b[0m     train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain_per_epoch\u001b[0;34m(train_loader, model, optimizer, scheduler, loss_fn, device, max_norm_grad)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx_0D : \u001b[39m\u001b[39m\"\u001b[39m, x_0D\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtarget : \u001b[39m\u001b[39m\"\u001b[39m, target\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=38'>39</a>\u001b[0m output \u001b[39m=\u001b[39m model(x_video, x_0D)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=39'>40</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_surromind/home/jinsukim/codes_for_study/research-predict-disruption/analysis_multi_modal_data.ipynb#ch0000015vscode-remote?line=41'>42</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes_for_study/research-predict-disruption/src/models/mult_modal.py:26\u001b[0m, in \u001b[0;36mMultiModalModel.forward\u001b[0;34m(self, x_video, x_0D)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_video : torch\u001b[39m.\u001b[39mTensor, x_0D : torch\u001b[39m.\u001b[39mTensor)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mtorch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     25\u001b[0m     x_video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_video(x_video)\n\u001b[0;32m---> 26\u001b[0m     x_0D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_0D(x_0D)\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x_video, x_0D], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes_for_study/research-predict-disruption/src/models/ConvLSTM.py:133\u001b[0m, in \u001b[0;36mConvLSTMEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x : torch\u001b[39m.\u001b[39mTensor)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mtorch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 133\u001b[0m     x_conv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    134\u001b[0m     h_0 \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m, x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_dim))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    135\u001b[0m     c_0 \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m, x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_dim))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:301\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/research-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:297\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    295\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    296\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 297\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    298\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 1e-3\n",
    "\n",
    "from src.loss import FocalLoss\n",
    "from src.models.mult_modal import MultiModalModel\n",
    "\n",
    "train_data.get_num_per_cls()\n",
    "cls_num_list = train_data.get_cls_num_list()\n",
    "per_cls_weights = 1.0 / np.array(cls_num_list)\n",
    "per_cls_weights = per_cls_weights / np.sum(per_cls_weights)\n",
    "per_cls_weights = torch.FloatTensor(per_cls_weights).to(device)\n",
    "loss_fn = FocalLoss(per_cls_weights, gamma = 2)\n",
    "\n",
    "args_video = {\n",
    "    \"image_size\" : 128, \n",
    "    \"patch_size\" : 32, \n",
    "    \"n_frames\" : 21, \n",
    "    \"dim\": 64, \n",
    "    \"depth\" : 4, \n",
    "    \"n_heads\" : 8, \n",
    "    \"pool\" : 'cls', \n",
    "    \"in_channels\" : 3, \n",
    "    \"d_head\" : 64, \n",
    "    \"dropout\" : 0.25,\n",
    "    \"embedd_dropout\":  0.25, \n",
    "    \"scale_dim\" : 4\n",
    "}\n",
    "\n",
    "args_0D = {\n",
    "    \"seq_len\" : 21, \n",
    "    \"col_dim\" : 9, \n",
    "    \"conv_dim\" : 32, \n",
    "    \"conv_kernel\" : 3,\n",
    "    \"conv_stride\" : 1, \n",
    "    \"conv_padding\" : 1,\n",
    "    \"lstm_dim\" : 64, \n",
    "}\n",
    "    \n",
    "model = MultiModalModel(\n",
    "    2, args_video, args_0D\n",
    ")\n",
    "model.summary('cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epoch = 64\n",
    "verbose = 4\n",
    "save_best_dir = \"./weights/multi_modal_best.pt\"\n",
    "save_last_dir = \"./weights/multi_modal_last.pt\"\n",
    "max_norm_grad = 1.0\n",
    "criteria = \"f1_score\"\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "train_loss, train_acc, train_f1, valid_loss, valid_acc, valid_f1 = train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    None,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    num_epoch,\n",
    "    verbose,\n",
    "    save_best_dir,\n",
    "    save_last_dir,\n",
    "    max_norm_grad,\n",
    "    criteria\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('research-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13691d44a992e9eda53412410cd635bb66f9b6a1d0122e1b07e8626f6211bdec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
