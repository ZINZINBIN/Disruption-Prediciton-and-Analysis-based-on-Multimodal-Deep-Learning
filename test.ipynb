{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from vision_transformer_pytorch import VisionTransformer\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer.from_name('ViT-B_16', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Literal\n",
    "\n",
    "def get_attention_map(img, get_mask=False):\n",
    "    x = transform(img)\n",
    "    x.size()\n",
    "\n",
    "    logits, att_mat = model(x.unsqueeze(0))\n",
    "\n",
    "    att_mat = torch.stack(att_mat).squeeze(1)\n",
    "\n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(att_mat, dim=1)\n",
    "\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    residual_att = torch.eye(att_mat.size(1))\n",
    "    aug_att_mat = att_mat + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "\n",
    "    v = joint_attentions[-1]\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "    if get_mask:\n",
    "        result = cv2.resize(mask / mask.max(), img.size)\n",
    "    else:        \n",
    "        mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n",
    "        result = (mask * img).astype(\"uint8\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def plot_attention_map(original_img, att_map):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "    ax1.set_title('Original')\n",
    "    ax2.set_title('Attention Map Last Layer')\n",
    "    _ = ax1.imshow(original_img)\n",
    "    _ = ax2.imshow(att_map)\n",
    "    \n",
    "def rollout(attentions : List[torch.Tensor], discard_ratio : float, head_fusion : Literal['mean', 'max', 'min']):\n",
    "    # attentions : List which consist of [1, channels, height, width] size of attention data\n",
    "    result = torch.eye(attentions[0].size(-1))\n",
    "    with torch.no_grad():\n",
    "        for attention in attentions:\n",
    "            if head_fusion == 'mean':\n",
    "                attention_heads_fused = attention.mean(axis = 1)\n",
    "            elif head_fusion == 'max':\n",
    "                attention_heads_fused = attention.max(axis = 1)[0]\n",
    "            elif head_fusion == 'min':\n",
    "                attention_heads_fused = attention.min(axis = 1)[0]\n",
    "            \n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size()[0], -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1) * discard_ratio), dim = -1, largest = False)\n",
    "            indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "            \n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0 * I) / 2.0\n",
    "            a = a / a.sum(dim = -1)\n",
    "            \n",
    "            result = torch.matmul(a, result)\n",
    "    \n",
    "    mask = result\n",
    "    mask = mask.numpy()\n",
    "    mask = mask / np.max(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"./dataset/dur21_dis0/test/disruption/21325_1369_1390/0000.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VITAttentionRollout:\n",
    "    def __init__(self, model : nn.Module, attention_layer_name = 'attn', head_fusion = 'mean', discard_ratio = 0.9):\n",
    "        self.model = model\n",
    "        self.head_fusion = head_fusion\n",
    "        self.discard_ratio = discard_ratio\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            for name_, module_ in module.named_children():\n",
    "                if attention_layer_name in name_:\n",
    "                    module.register_forward_hook(self.get_attention)\n",
    "        \n",
    "        self.attentions = []\n",
    "    \n",
    "    def get_attention(self, module, input, output):\n",
    "        self.attentions.append(output.cpu())\n",
    "    \n",
    "    def __call__(self, input_tensor : torch.Tensor):\n",
    "        self.attentions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "        \n",
    "        return rollout(self.attentions, self.discard_ratio, self.head_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "x = transform(img).to(device).unsqueeze(0)\n",
    "model.to(device)\n",
    "\n",
    "rollout_model = VITAttentionRollout(model)\n",
    "att_mask = rollout_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Input argument doesn't provide sequence protocol\n>  - Can't parse 'dsize'. Input argument doesn't provide sequence protocol\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdiya_external/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     _ \u001b[39m=\u001b[39m ax1\u001b[39m.\u001b[39mimshow(original_img)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdiya_external/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     _ \u001b[39m=\u001b[39m ax2\u001b[39m.\u001b[39mimshow(att_map)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdiya_external/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m plot_attention_map(img, att_mask)\n",
      "\u001b[1;32m/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb Cell 7\u001b[0m in \u001b[0;36mplot_attention_map\u001b[0;34m(original_img, att_map)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdiya_external/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_attention_map\u001b[39m(original_img, att_map):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdiya_external/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     att_map \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(\u001b[39m384\u001b[39;49m,\u001b[39m384\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdiya_external/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     fig, (ax1, ax2) \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(ncols\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m16\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdiya_external/home/jinsukim/codes_for_study/research-predict-disruption/test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     ax1\u001b[39m.\u001b[39mset_title(\u001b[39m'\u001b[39m\u001b[39mOriginal\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Input argument doesn't provide sequence protocol\n>  - Can't parse 'dsize'. Input argument doesn't provide sequence protocol\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"./dataset/dur21_dis0/test/disruption/21325_1369_1390/0000.jpg\")\n",
    "img = transform(img).numpy()\n",
    "\n",
    "def plot_attention_map(original_img, att_map):\n",
    "    att_map = cv2.resize(384, 384)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "    ax1.set_title('Original')\n",
    "    ax2.set_title('Attention Map Last Layer')\n",
    "    _ = ax1.imshow(original_img)\n",
    "    _ = ax2.imshow(att_map)\n",
    "\n",
    "plot_attention_map(img, att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384, 384)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('research-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13691d44a992e9eda53412410cd635bb66f9b6a1d0122e1b07e8626f6211bdec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
